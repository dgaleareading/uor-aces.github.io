
@inproceedings{OspEA13,
  timestamp = {2018-01-22T15:22:53Z},
  location = {{Las Vegas}},
  title = {A {{Benchmark}}-{{Driven Modelling Approach}} for {{Evaluating Deployment Choices}} on a {{Multicore Architecture}}},
  isbn = {1-60132-258-5},
  booktitle = {Proceedings of the {{International Conference}} on {{Parallel}} \& {{Distributed Processing Techniques}} \& {{Application}} ({{PDPTA}}'13)},
  author = {Osprey, A. and Riley, G.D. and Manjunathaiah, M. and Lawrence, B.N.},
  date = {2013-07},
  pages = {571--577},
  keywords = {bnl-cv,bnl-cv-refconf,bnl-cv-refgen,aces},
  file = {PDP3558.pdf:/Users/BNL28/zotero-storage/storage/MF9AAZ8C/PDP3558.pdf:application/pdf}
}

@inproceedings{OspEA14,
  timestamp = {2018-02-04T20:16:31Z},
  title = {The Development of a Data-Driven Application Benchmarking Approach to Performance Modelling},
  doi = {10.1109/HPCSim.2014.6903760},
  abstract = {Performance modelling is a useful tool in the lifeycle of high performance scientific software, such as weather and climate models, especially as a means of ensuring efficient use of available computing resources. In particular, sufficiently accurate performance prediction could reduce the effort and experimental computer time required when porting and optimising a climate model to a new machine. Yet as architectures become more complex, performance prediction is becoming more difficult. Traditional methods of performance prediction, based on source code analysis and supported by machine benchmarks, are proving inadequate to the task. In this paper, the reasons for this are explored by applying some traditional techniques to predict the computation time of a simple shallow water model which is illustrative of the computation (and communication) involved in climate models. These models are compared with real execution data gathered on AMD Opteron-based systems, including several phases of the U.K. academic community HPC resource, HECToR. Some success is had in relating source code to achieved performance for the K10 series of Opterons, but the method is found to be inadequate for the next-generation Interlagos processor. The experience leads to the investigation of a data-driven application benchmarking approach to performance modelling. Results for an early version of the approach are presented using the shallow model as an example. In addition, the data-driven approach is compared with a novel analytical model based on fitting logarithmic curves to benchmarked application data. The limitations of this analytical method provide further motivation for the development of the data-driven approach and results of this work have been published elsewhere.},
  eventtitle = {2014 {{International Conference}} on {{High Performance Computing Simulation}} ({{HPCS}})},
  booktitle = {2014 {{International Conference}} on {{High Performance Computing Simulation}} ({{HPCS}})},
  author = {Osprey, A. and Riley, G.D. and Manjunathaiah, M. and Lawrence, B.N.},
  date = {2014-07},
  pages = {715--723},
  keywords = {bnl-cv,Bandwidth,parallel processing,geophysics computing,Climate model,Meteorology,Computer architecture,Benchmark testing,curve fitting,source code (software),AMD Opteron-based systems,HECToR,UK academic community HPC resource,data-driven application benchmarking approach development,high performance scientific software lifeycle,logarithmic curve fitting,machine benchmarks,next-generation Interlagos processor,performance modelling,performance prediction,shallow water model,source code analysis,Analytical models,Computational modeling,Mathematical model,benchmarking,multicore,bnl-cv-refconf,bnl-cv-refgen,aces},
  file = {IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/766H76DP/abstractAuthors.html:text/html}
}

@unpublished{KunEA16,
  timestamp = {2018-08-31T09:18:39Z},
  location = {{Salt Lake City}},
  title = {Middleware for {{Earth System Data}}},
  url = {http://www.pdsw.org/pdsw-discs16/wips/kunkel1-wip-pdsw-discs16.pdf},
  type = {Work in {{Progress}}},
  howpublished = {Work in Progress},
  eventtitle = {1st {{International Workshop}} on {{Parallell Data Storage}} and {{Data Intensive Scalable Computing Systems}} ({{PDSW}}-{{DISCS}}'16)},
  author = {Kunkel, Julian and Luettgau, Jakob and Lawrence, Bryan N. and Jensen, Jens and Congiu, Giuseppe and Readey, John},
  urldate = {2017-06-23},
  date = {2016},
  keywords = {bnl,prp17,bnl-cv-conf,aces,prp18},
  file = {kunkel1-wip-pdsw-discs16.pdf:/Users/BNL28/zotero-storage/storage/PE3Z3XWQ/kunkel1-wip-pdsw-discs16.pdf:application/pdf}
}

@article{BalEA17p,
  timestamp = {2018-08-31T09:18:18Z},
  title = {{{CPMIP}}: Measurements of Real Computational Performance of {{Earth}} System Models in {{CMIP6}}},
  volume = {10},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-19-2017},
  shorttitle = {{{CPMIP}}},
  abstract = {A climate model represents a multitude of processes on a variety of timescales and space scales: a canonical example of multi-physics multi-scale modeling. The underlying climate system is physically characterized by sensitive dependence on initial conditions, and natural stochastic variability, so very long integrations are needed to extract signals of climate change. Algorithms generally possess weak scaling and can be I/O and/or memory-bound. Such weak-scaling, I/O, and memory-bound multi-physics codes present particular challenges to computational performance.  Traditional metrics of computational efficiency such as performance counters and scaling curves do not tell us enough about real sustained performance from climate models on different machines. They also do not provide a satisfactory basis for comparative information across models. codes present particular challenges to computational performance.  We introduce a set of metrics that can be used for the study of computational performance of climate (and Earth system) models. These measures do not require specialized software or specific hardware counters, and should be accessible to anyone. They are independent of platform and underlying parallel programming models. We show how these metrics can be used to measure actually attained performance of Earth system models on different machines, and identify the most fruitful areas of research and development for performance engineering. codes present particular challenges to computational performance.  We present results for these measures for a diverse suite of models from several modeling centers, and propose to use these measures as a basis for a CPMIP, a computational performance model intercomparison project (MIP).},
  number = {1},
  journaltitle = {Geosci. Model Dev.},
  shortjournal = {Geosci. Model Dev.},
  author = {Balaji, V. and Maisonnave, E. and Zadeh, N. and Lawrence, B. N. and Biercamp, J. and Fladrich, U. and Aloisio, G. and Benson, R. and Caubel, A. and Durachta, J. and Foujols, M.-A. and Lister, G. and Mocavero, S. and Underwood, S. and Wright, G.},
  date = {2017-01-02},
  pages = {19--34},
  keywords = {bnl,bnl-cv,chasm,prp17,bnl-cv-refgen,bnl-cv-refjnl,onblog,aces,prp18},
  file = {Geosci. Model Dev. PDF:/Users/BNL28/zotero-storage/storage/N79RNUKT/Balaji et al. - 2017 - CPMIP measurements of real computational performa.pdf:application/pdf}
}

@inproceedings{MasEA17,
  timestamp = {2018-02-13T10:55:08Z},
  location = {{Toulouse, France}},
  title = {Evolving {{JASMIN}}: {{High}} Performance Analysis and the Data Deluge},
  doi = {10.2760/383579},
  abstract = {JASMIN is a highly successful data analysis system, which is used by thousands of academics and their industrial partners to analyse many petabytes of environmental data. The rapidly increasing volume of data stored on JASMIN, and the steadily increasing number of users, is making it necessary to investigate and implement new methods of providing computing resources to the users, storing the data that they produce from their analyses and storing and maintaining a very large archive of environmental data. To achieve this, two main areas of research are described. Firstly, providing users with virtualised services to best utilise the computing resources available. Secondly, using object storage to provide a large, yet affordable, data store and providing the users with tools and interfaces to common environmental data formats, so as to not unduly affect their current work flows.},
  booktitle = {Proc. of the 2017 Conference on {{Big Data}} from {{Space}} ({{BiDS}}’17)},
  author = {Massey, Neil and Kershaw, Philip and Pritchard, Matt and Pryor, Matt and Pepler, Sam and Churchill, Jonathan and Lawrence, Bryan},
  date = {2017},
  pages = {287--288},
  keywords = {bnl,bnl-cv,bnl-cv-conf,aces},
  file = {MasEA17.pdf:/Users/BNL28/zotero-storage/storage/63GBYWD2/MasEA17.pdf:application/pdf}
}

@unpublished{LueEA17,
  timestamp = {2018-02-13T10:54:29Z},
  location = {{Denver}},
  title = {Towards {{Structure}}-{{Aware Earth System Data Management}}},
  abstract = {Current storage environments confront domain scientist and data
center operators with usability and performance challenges. To
achieve performance portability data description libraries such as
HDF5 and NetCDF are widely adopted. At the moment, these libraries
struggle to adequately account for access patterns when
reading and writing data to multi-tier distributed storage systems.
As part of the ESiWACE[1] project, we develop a novel I/O middleware
targeting, but not limited to, earth system data. The architecture
builds on top of well established end-user interfaces but utilizes
scientific metadata to harness a data structure centric perspective.},
  type = {Work in {{Progress}}},
  howpublished = {Work in Progress},
  eventtitle = {2nd {{International Workshop}} on {{Parallel Data Storage}} and {{Data Intensive Scalable Computing Systems}} ({{PDSW}}-{{DISCS}}'17)},
  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan N. and Fiore, Sandro and Hua, Huang},
  date = {2017-11-13},
  keywords = {bnl,bnl-cv,bnl-cv-conf,aces},
  file = {LueEA17.pdf:/Users/BNL28/zotero-storage/storage/YQUWLZ4C/LueEA17.pdf:application/pdf}
}

@article{HubbeReducingHPCdatastorageFootprint2013,
  timestamp = {2018-02-20T19:11:33Z},
  title = {Reducing the {{HPC}}-Datastorage {{Footprint}} with {{MAFISC}}–{{Multidimensional Adaptive Filtering Improved Scientific Data Compression}}},
  volume = {28},
  issn = {1865-2034},
  url = {http://dx.doi.org/10.1007/s00450-012-0222-4},
  doi = {10.1007/s00450-012-0222-4},
  abstract = {Large HPC installations today also include large data storage installations. Data compression can significantly reduce the amount of data, and it was one of our goals to find out, how much compression can do for climate data. The price of compression is, of course, the need for additional computational resources, so our second goal was to relate the savings of compression to the costs it necessitates.In this paper we present the results of our analysis of typical climate data. A lossless algorithm based on these insights is developed and its compression ratio is compared to that of standard compression tools. As it turns out, this algorithm is general enough to be useful for a large class of scientific data, which is the reason we speak of MAFISC as a method for scientific data compression. A numeric problem for lossless compression of scientific data is identified and a possible solution is given. Finally, we discuss the economics of data compression in HPC environments using the example of the German Climate Computing Center.},
  issue = {2-3},
  journaltitle = {Comput. Sci.},
  author = {Hübbe, Nathanael and Kunkel, Julian},
  urldate = {2018-02-20},
  date = {2013-05},
  pages = {231--239},
  keywords = {NetCDF,aces,Data compression,HDF5}
}

@article{HubbeEvaluatinglossycompression2013,
  timestamp = {2018-02-20T19:21:06Z},
  title = {Evaluating Lossy Compression on Climate Data},
  volume = {7905},
  doi = {10.1007/978-3-642-38750-0_26},
  journaltitle = {International Supercomputing Conference},
  series = {Lecture Notes in Computer Science},
  author = {Hübbe, Nathanael and Wegener, Al and Kunkel, Julian Martin and Ling, Yi and Ludwig, Thomas},
  date = {2013},
  pages = {343--356},
  keywords = {aces},
  file = {e0ef270f5cff61e2f12b2f2068172554854a.pdf:/Users/BNL28/zotero-storage/storage/MXVI9G3A/e0ef270f5cff61e2f12b2f2068172554854a.pdf:application/pdf}
}

@article{KuhnDynamicfilesystem2009,
  timestamp = {2018-02-20T19:13:11Z},
  langid = {english},
  title = {Dynamic File System Semantics to Enable Metadata Optimizations in {{PVFS}}},
  volume = {21},
  issn = {15320626, 15320634},
  url = {http://doi.wiley.com/10.1002/cpe.1439},
  doi = {10.1002/cpe.1439},
  number = {14},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  author = {Kuhn, Michael and Kunkel, Julian Martin and Ludwig, Thomas},
  urldate = {2018-02-20},
  date = {2009-09-25},
  pages = {1775--1788},
  keywords = {aces},
  file = {318b3288506266db1c02377022f4a067c4c9.pdf:/Users/BNL28/zotero-storage/storage/MXZG64KD/318b3288506266db1c02377022f4a067c4c9.pdf:application/pdf}
}

@inproceedings{Meisterstudydatadeduplication2012a,
  timestamp = {2018-02-20T19:23:41Z},
  title = {A Study on Data Deduplication in {{HPC}} Storage Systems},
  doi = {10.1109/SC.2012.14},
  abstract = {Deduplication is a storage saving technique that is highly successful in enterprise backup environments. On a file system, a single data block might be stored multiple times across different files, for example, multiple versions of a file might exist that are mostly identical. With deduplication, this data replication is localized and redundancy is removed – by storing data just once, all files that use identical regions refer to the same unique data. The most common approach splits file data into chunks and calculates a cryptographic fingerprint for each chunk. By checking if the fingerprint has already been stored, a chunk is classified as redundant or unique. Only unique chunks are stored. This paper presents the first study on the potential of data deduplication in HPC centers, which belong to the most demanding storage producers. We have quantitatively assessed this potential for capacity reduction for 4 data centers (BSC, DKRZ, RENCI, RWTH). In contrast to previous deduplication studies focusing mostly on backup data, we have analyzed over one PB (1212 TB) of online file system data. The evaluation shows that typically 20\% to 30\% of this online data can be removed by applying data deduplication techniques, peaking up to 70\% for some data sets. This reduction can only be achieved by a subfile deduplication approach, while approaches based on whole-file comparisons only lead to small capacity savings.},
  eventtitle = {High {{Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} ({{SC}}), 2012 {{International Conference}} For},
  booktitle = {High {{Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} ({{SC}}), 2012 {{International Conference}} For},
  author = {Meister, D. and Kaiser, J. and Brinkmann, A. and Cortes, T. and Kuhn, M. and Kunkel, J.},
  date = {2012-11},
  pages = {1--11},
  keywords = {Educational institutions,Internet,Meteorology,Virtual machining,storage management,Indexes,aces,back-up procedures,backup data,BSC,capacity reduction,computer centres,cryptographic fingerprint calculation,cryptography,Cryptography,data block,data centers,data deduplication,DKRZ,enterprise backup environments,file data,Focusing,HPC centers,HPC storage system,localized data replication,online file system data,Redundancy,redundancy removal,RENCI,replicated databases,RWTH,storage saving technique,subfile deduplication approach},
  file = {Meister et al. - 2012 - A study on data deduplication in HPC storage syste.pdf:/Users/BNL28/zotero-storage/storage/XBUQEK35/Meister et al. - 2012 - A study on data deduplication in HPC storage syste.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/ZFI96DPP/6468447.html:text/html}
}

@inproceedings{CarnsSmallfileaccessparallel2009a,
  timestamp = {2018-02-20T19:26:10Z},
  title = {Small-File Access in Parallel File Systems},
  doi = {10.1109/IPDPS.2009.5161029},
  abstract = {Today's computational science demands have resulted in ever larger parallel computers, and storage systems have grown to match these demands. Parallel file systems used in this environment are increasingly specialized to extract the highest possible performance for large I/O operations, at the expense of other potential workloads. While some applications have adapted to I/O best practices and can obtain good performance on these systems, the natural I/O patterns of many applications result in generation of many small files. These applications are not well served by current parallel file systems at very large scale. This paper describes five techniques for optimizing small-file access in parallel file systems for very large scale systems. These five techniques are all implemented in a single parallel file system (PVFS) and then systematically assessed on two test platforms. A microbenchmark and the mdtest benchmark are used to evaluate the optimizations at an unprecedented scale. We observe as much as a 905\% improvement in small-file create rates, 1,106\% improvement in small-file stat rates, and 727\% improvement in small-file removal rates, compared to a baseline PVFS configuration on a leadership computing platform using 16,384 cores.},
  eventtitle = {2009 {{IEEE International Symposium}} on {{Parallel Distributed Processing}}},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel Distributed Processing}}},
  author = {Carns, P. and Lang, S. and Ross, R. and Vilayannur, M. and Kunkel, J. and Ludwig, T.},
  date = {2009-05},
  pages = {1--11},
  keywords = {Laboratories,Application software,Computer science,Mathematics,parallel processing,Benchmark testing,Large-scale systems,file organisation,Concurrent computing,parallel file system,aces,Best practices,File systems,large I/O operation,small-file access,small-file create rate,small-file removal rate,small-file stat rate,System testing,very large scale system},
  file = {Carns et al. - 2009 - Small-file access in parallel file systems.pdf:/Users/BNL28/zotero-storage/storage/SCHYZ23A/Carns et al. - 2009 - Small-file access in parallel file systems.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/HWA2H6TW/5161029.html:text/html}
}

@article{LawChasm,
  timestamp = {2018-08-31T09:19:19Z},
  langid = {english},
  title = {Crossing the Chasm: How to Develop Weather and Climate Models for next Generation Computers?},
  volume = {11},
  issn = {1991-9603},
  url = {https://www.geosci-model-dev.net/11/1799/2018/},
  doi = {10.5194/gmd-11-1799-2018},
  shorttitle = {Crossing the Chasm},
  number = {5},
  journaltitle = {Geoscientific Model Development},
  author = {Lawrence, Bryan N. and Rezny, Michael and Budich, Reinhard and Bauer, Peter and Behrens, Jörg and Carter, Mick and Deconinck, Willem and Ford, Rupert and Maynard, Christopher and Mullerworth, Steven and Osuna, Carlos and Porter, Andrew and Serradell, Kim and Valcke, Sophie and Wedi, Nils and Wilson, Simon},
  urldate = {2018-05-08},
  date = {2018-05-08},
  pages = {1799--1821},
  keywords = {bnl-cv,bnl-cv-refgen,bnl-cv-refjnl,easc2018,aces,prp18}
}

@article{NabeehJumahGGDMLIcosahedralModels2017,
  timestamp = {2018-05-08T08:04:55Z},
  title = {{{GGDML}}: {{Icosahedral Models Language Extensions}}},
  volume = {4},
  issn = {24102938},
  url = {http://www.cosmosscholars.com/current-issue-jcstu/79-abstracts/jcstu/708-abstract-ggdml-icosahedral-models-language-extensions},
  doi = {10.15379/2410-2938.2017.04.01.01},
  shorttitle = {{{GGDML}}},
  abstract = {The optimization opportunities of a code base are not completely exploited by compilers. In fact, there are optimizations that must be done within the source code. Hence, if the code developers skip some details, some performance is lost. Thus, the use of a general-purpose language to develop a performance-demanding software -e.g. climate models- needs more care from the developers. They should take into account hardware details of the target machine.  Besides, writing a high-performance code for one machine will have a lower performance on another one. The developers usually write multiple optimized sections or even code versions for the different target machines. Such codes are complex and hard to maintain.  In this article we introduce a higher-level code development approach, where we develop a set of extensions to the language that is used to write a model’s code. Our extensions form a domain-specific language (DSL) that abstracts domain concepts and leaves the lower level details to a configurable source-to-source translation process.  The purpose of the developed extensions is to support the icosahedral climate/atmospheric model development. We have started with the three icosahedral models: DYNAMICO, ICON, and NICAM. The collaboration with the scientists from the weather/climate sciences enabled agreed-upon extensions. When we have suggested an extension we kept in mind that it represents a higher-level domain-based concept, and that it carries no lower-level details.  The introduced DSL (GGDML- General Grid Definition and Manipulation Language) hides optimization details like memory layout. It reduces code size of a model to less than one third its original size in terms of lines of code. The development costs of a model with GGDML are therefore reduced significantly.},
  number = {1},
  journaltitle = {Journal of Computer Science Technology Updates},
  author = {{Nabeeh Jumah} and {Julian Kunkel} and {Günther Zängel} and {Hisashi Yashiro} and {Thomas Dubos} and {Yann Meurdesoif}},
  urldate = {2018-05-08},
  date = {2017-06-22},
  pages = {1--10},
  keywords = {aces}
}

@article{KunkelDecouplingSelectionCompression2017,
  timestamp = {2018-05-08T08:12:03Z},
  langid = {english},
  title = {Towards {{Decoupling}} the {{Selection}} of {{Compression Algorithms}} from {{Quality Constraints}} – {{An Investigation}} of {{Lossy Compression Efficiency}}},
  volume = {4},
  rights = {Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a~ Creative Commons Attribution License ~that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.},
  issn = {2313-8734},
  url = {http://www.superfri.org/superfri/article/view/149},
  doi = {10.14529/jsfi170402},
  abstract = {Data intense scientific domains use data compression to reduce the storage space needed. Lossless data compression preserves information accurately but lossy data compression can achieve much higher compression rates depending on the tolerable error margins. There are many ways of defining precision and to exploit this knowledge, therefore, the field of lossy compression is subject to active research. From the perspective of a scientist, the qualitative definition about the implied loss of data precision should only matter.With the Scientific Compression Library (SCIL), we are developing a meta-compressor that allows users to define various quantities for acceptable error and expected performance behavior. The library then picks a suitable chain of algorithms yielding the user’s requirements, the ongoing work is a preliminary stage for the design of an adaptive selector. This approach is a crucial step towards a scientifically safe use of much-needed lossy data compression, because it disentangles the tasks of determining scientific characteristics of tolerable noise, from the task of determining an optimal compression strategy. Future algorithms can be used without changing application code.In this paper, we evaluate various lossy compression algorithms for compressing different scientific datasets (Isabel, ECHAM6), and focus on the analysis of synthetically created data that serves as blueprint for many observed datasets. We also briefly describe the available quantitiesof SCIL to define data precision and introduce two efficient compression algorithms for individualdata points. This shows that the best algorithm depends on user settings and data properties.},
  number = {4},
  journaltitle = {Supercomputing Frontiers and Innovations},
  author = {Kunkel, Julian Martin and Novikova, Anastasiia and Betke, Eugen},
  urldate = {2018-05-08},
  date = {2017-11-21},
  pages = {17--33},
  keywords = {aces},
  file = {Kunkel et al. - 2017 - Towards Decoupling the Selection of Compression Al.pdf:/Users/BNL28/zotero-storage/storage/6X2WKDY7/Kunkel et al. - 2017 - Towards Decoupling the Selection of Compression Al.pdf:application/pdf;Snapshot:/Users/BNL28/zotero-storage/storage/AL7385SY/149.html:text/html}
}

@inproceedings{RaoultFastRetrievalWeather2018,
  timestamp = {2018-06-14T07:04:41Z},
  langid = {english},
  title = {Fast {{Retrieval}} of {{Weather Analogues}} in a {{Multi}}-Petabytes {{Archive Using Wavelet}}-{{Based Fingerprints}}},
  isbn = {978-3-319-93700-7 978-3-319-93701-4},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-93701-4_55},
  doi = {10.1007/978-3-319-93701-4_55},
  abstract = {Very large climate data repositories provide a consistent view of weather conditions over long time periods. In some applications and studies, given a current weather pattern (e.g. today’s weather), it is useful to identify similar ones (weather analogues) in the past. Looking for similar patterns in an archive using a brute force approach requires data to be retrieved from the archive and then compared to the query, using a chosen similarity measure. Such operation would be very long and costly. In this work, a wavelet-based fingerprinting scheme is proposed to index all weather patterns from the archive. The scheme allows to answer queries by computing the fingerprint of the query pattern, then comparing them to the index of all fingerprints more efficiently, in order to then retrieve only the corresponding selected data from the archive. The experimental analysis is carried out on the ECMWF’s ERA-Interim reanalyses data representing the global state of the atmosphere over several decades. Results shows that 32 bits fingerprints are sufficient to represent meteorological fields over a 1700 km ××\{$\backslash$times \} 1700 km region and allow the quasi instantaneous retrieval of weather analogues.},
  eventtitle = {International {{Conference}} on {{Computational Science}}},
  booktitle = {Computational {{Science}} – {{ICCS}} 2018},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  author = {Raoult, Baudouin and Fatta, Giuseppe Di and Pappenberger, Florian and Lawrence, Bryan},
  urldate = {2018-06-13},
  date = {2018-06-11},
  pages = {697--710},
  keywords = {bnl,bnl-cv,bnl-cv-refgen,aces},
  file = {Raoult et al. - 2018 - Fast Retrieval of Weather Analogues in a Multi-pet.pdf:/Users/BNL28/zotero-storage/storage/3DPJU4X5/Raoult et al. - 2018 - Fast Retrieval of Weather Analogues in a Multi-pet.pdf:application/pdf;Snapshot:/Users/BNL28/zotero-storage/storage/JKARVPSH/978-3-319-93701-4_55.html:text/html}
}

@article{BalEA18,
  timestamp = {2018-09-23T17:03:52Z},
  langid = {english},
  title = {Requirements for a Global Data Infrastructure in Support of {{CMIP6}}},
  volume = {11},
  url = {https://www.geosci-model-dev.net/11/3659/2018/gmd-11-3659-2018.pdf},
  doi = {https://doi.org/10.5194/gmd-11-3659-2018},
  abstract = {The World Climate Research Programme (WCRP)’s Working Group on Climate Modelling (WGCM) Infrastructure Panel (WIP) was formed in 2014 in response to the explosive growth in size and complexity of Coupled Model Intercomparison Projects (CMIPs) between CMIP3 (2005–2006) and CMIP5 (2011–2012). This article presents the WIP recommendations for the global data infrastructure needed to support CMIP design, future growth, and evolution. Developed in close coordination with those who build and run the existing infrastructure (the Earth System Grid Federation; ESGF), the recommendations are based on several principles beginning with the need to separate requirements, implementation, and operations. Other important principles include the consideration of the diversity of community needs around data – a data ecosystem – the importance of provenance, the need for automation, and the obligation to measure costs and beneﬁts.},
  journaltitle = {Geoscientific Model Development},
  author = {Balaji, Venkatramani and Taylor, Karl E. and Juckes, Martin and Lawrence, Bryan N. and Durack, Paul J. and Lautenschlager, Michael and Blanton, Chris and Cinquini, Luca and Denvil, Sebastien and Elkington, Mark and Guglielmo, Francesca and Guilyardi, Eric and Hassell, David and Kharin, Slava and Kindermann, Stefan and Nikonov, Sergey and Radhakrishnan, Aparna and Stockhause, Martina and Weigel, Tobias and Williams, Dean},
  urldate = {2018-09-12},
  date = {2018-09-11},
  pages = {3659--3680},
  keywords = {bnl-cv,bnl-cv-refgen,bnl-cv-refjnl,aces,semsl_paper,prp18},
  file = {Balaji et al. - 2018 - Requirements for a global data infrastructure in s.pdf:/Users/BNL28/zotero-storage/storage/AQBL7RME/Balaji et al. - 2018 - Requirements for a global data infrastructure in s.pdf:application/pdf}
}


